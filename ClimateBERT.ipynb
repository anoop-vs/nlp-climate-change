{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43f93d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "tweets = pd.read_csv('main_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7cb93ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Researchers use deep learning to simulate chlo...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Why is our @Conservatives government so evil?\\...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Texas Oilfield Waste Company Contributed $53,7...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Epic California snowpack is now the deepest it...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>If #climatechange is real and not a hoax why d...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4405</th>\n",
       "      <td>353</td>\n",
       "      <td>['𝐃𝐚𝐭𝐚 𝐃𝐫𝐢𝐯𝐞𝐧 𝐃𝐄𝐈 with @ team73bit (73bit. com...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4406</th>\n",
       "      <td>354</td>\n",
       "      <td>['𝐃𝐚𝐭𝐚 𝐃𝐫𝐢𝐯𝐞𝐧 𝐃𝐄𝐈 with @ team73bit (73bit. com...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4407</th>\n",
       "      <td>355</td>\n",
       "      <td>['Motivation direction on how to discover # su...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4408</th>\n",
       "      <td>356</td>\n",
       "      <td>['𝐃𝐚𝐭𝐚 𝐃𝐫𝐢𝐯𝐞𝐧 𝐃𝐄𝐈 with @ team73bit (73bit. com...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4409</th>\n",
       "      <td>357</td>\n",
       "      <td>['AnthropoceneDAO. com » T - Wandering Announc...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4410 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                            content     label\n",
       "0              0  Researchers use deep learning to simulate chlo...  Positive\n",
       "1              1  Why is our @Conservatives government so evil?\\...  Negative\n",
       "2              2  Texas Oilfield Waste Company Contributed $53,7...  Negative\n",
       "3              3  Epic California snowpack is now the deepest it...  Negative\n",
       "4              4  If #climatechange is real and not a hoax why d...  Negative\n",
       "...          ...                                                ...       ...\n",
       "4405         353  ['𝐃𝐚𝐭𝐚 𝐃𝐫𝐢𝐯𝐞𝐧 𝐃𝐄𝐈 with @ team73bit (73bit. com...   Neutral\n",
       "4406         354  ['𝐃𝐚𝐭𝐚 𝐃𝐫𝐢𝐯𝐞𝐧 𝐃𝐄𝐈 with @ team73bit (73bit. com...   Neutral\n",
       "4407         355  ['Motivation direction on how to discover # su...  Positive\n",
       "4408         356  ['𝐃𝐚𝐭𝐚 𝐃𝐫𝐢𝐯𝐞𝐧 𝐃𝐄𝐈 with @ team73bit (73bit. com...   Neutral\n",
       "4409         357  ['AnthropoceneDAO. com » T - Wandering Announc...  Positive\n",
       "\n",
       "[4410 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f757b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "486f11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the oversampler\n",
    "oversampler = RandomOverSampler(sampling_strategy={'Negative': 1842, 'Positive': 1822, 'Neutral': 1842})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69e163e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the features and the label\n",
    "X = tweets.drop('label', axis=1)\n",
    "y = tweets['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36a8805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample the negative class\n",
    "X_resampled, y_resampled = oversampler.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f978387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the updated dataframe\n",
    "tweets_oversampled = pd.concat([X_resampled, y_resampled], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b486428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Researchers use deep learning to simulate chlo...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Why is our @Conservatives government so evil?\\...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Texas Oilfield Waste Company Contributed $53,7...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Epic California snowpack is now the deepest it...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>If #climatechange is real and not a hoax why d...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5501</th>\n",
       "      <td>141</td>\n",
       "      <td>[\"Abstemious Contamination Is Dimming Our View...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5502</th>\n",
       "      <td>3</td>\n",
       "      <td>[\"Epic California snowpack personify now the d...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5503</th>\n",
       "      <td>359</td>\n",
       "      <td>Social value covers things that impact on peop...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504</th>\n",
       "      <td>71</td>\n",
       "      <td>['Climate variety may cut uracil forest stockt...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505</th>\n",
       "      <td>968</td>\n",
       "      <td>['We all strive for # sustainability and being...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5506 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                            content     label\n",
       "0              0  Researchers use deep learning to simulate chlo...  Positive\n",
       "1              1  Why is our @Conservatives government so evil?\\...  Negative\n",
       "2              2  Texas Oilfield Waste Company Contributed $53,7...  Negative\n",
       "3              3  Epic California snowpack is now the deepest it...  Negative\n",
       "4              4  If #climatechange is real and not a hoax why d...  Negative\n",
       "...          ...                                                ...       ...\n",
       "5501         141  [\"Abstemious Contamination Is Dimming Our View...  Negative\n",
       "5502           3  [\"Epic California snowpack personify now the d...  Negative\n",
       "5503         359  Social value covers things that impact on peop...  Negative\n",
       "5504          71  ['Climate variety may cut uracil forest stockt...  Negative\n",
       "5505         968  ['We all strive for # sustainability and being...  Negative\n",
       "\n",
       "[5506 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22c03df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label', ylabel='count'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a countplot of the labels\n",
    "sns.countplot(x='label', data=tweets_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a927b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to numeric representation\n",
    "label_dict = {'Positive': 0, 'Negative': 1, 'Neutral': 2}\n",
    "tweets_oversampled['label'] = tweets_oversampled['label'].apply(lambda x: label_dict[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "684fad2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Researchers use deep learning to simulate chlo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Why is our @Conservatives government so evil?\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Texas Oilfield Waste Company Contributed $53,7...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Epic California snowpack is now the deepest it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>If #climatechange is real and not a hoax why d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5501</th>\n",
       "      <td>141</td>\n",
       "      <td>[\"Abstemious Contamination Is Dimming Our View...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5502</th>\n",
       "      <td>3</td>\n",
       "      <td>[\"Epic California snowpack personify now the d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5503</th>\n",
       "      <td>359</td>\n",
       "      <td>Social value covers things that impact on peop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504</th>\n",
       "      <td>71</td>\n",
       "      <td>['Climate variety may cut uracil forest stockt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505</th>\n",
       "      <td>968</td>\n",
       "      <td>['We all strive for # sustainability and being...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5506 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                            content  label\n",
       "0              0  Researchers use deep learning to simulate chlo...      0\n",
       "1              1  Why is our @Conservatives government so evil?\\...      1\n",
       "2              2  Texas Oilfield Waste Company Contributed $53,7...      1\n",
       "3              3  Epic California snowpack is now the deepest it...      1\n",
       "4              4  If #climatechange is real and not a hoax why d...      1\n",
       "...          ...                                                ...    ...\n",
       "5501         141  [\"Abstemious Contamination Is Dimming Our View...      1\n",
       "5502           3  [\"Epic California snowpack personify now the d...      1\n",
       "5503         359  Social value covers things that impact on peop...      1\n",
       "5504          71  ['Climate variety may cut uracil forest stockt...      1\n",
       "5505         968  ['We all strive for # sustainability and being...      1\n",
       "\n",
       "[5506 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac42e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "train_df = tweets_oversampled.sample(frac=0.8, random_state=42)\n",
    "test_df = tweets_oversampled.drop(train_df.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5725a477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50500, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50500, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "#tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base', normalization=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"climatebert/distilroberta-base-climate-f\", normalization=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"climatebert/distilroberta-base-climate-f\",num_labels=3)\n",
    "#model = AutoModelForSequenceClassification.from_pretrained('vinai/bertweet-base', num_labels=3)\n",
    "\n",
    "# Set device\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5e73244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the data\n",
    "train_encoded_data = tokenizer.batch_encode_plus(train_df['content'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "train_input_ids = train_encoded_data['input_ids'].to(device)\n",
    "train_attention_mask = train_encoded_data['attention_mask'].to(device)\n",
    "train_labels = torch.tensor(train_df['label'].tolist()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78ffd10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded_data = tokenizer.batch_encode_plus(test_df['content'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "test_input_ids = test_encoded_data['input_ids'].to(device)\n",
    "test_attention_mask = test_encoded_data['attention_mask'].to(device)\n",
    "test_labels = torch.tensor(test_df['label'].tolist()).to(device)\n",
    "\n",
    "# Set optimizer and learning rate\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da803953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 332])\n",
      "torch.Size([16, 332])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (5312) to match target batch_size (16).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_attention_mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_labels\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     17\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1117\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1116\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m-> 1117\u001b[0m     masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1120\u001b[0m     output \u001b[38;5;241m=\u001b[39m (prediction_scores,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (5312) to match target batch_size (16)."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 4\n",
    "model.train()\n",
    "batch_size = 16\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(train_df), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        batch_input_ids = train_input_ids[i:i+batch_size]\n",
    "        batch_attention_mask = train_attention_mask[i:i+batch_size]\n",
    "        batch_labels = train_labels[i:i+batch_size]\n",
    "        print(batch_input_ids.shape)\n",
    "        print(batch_attention_mask.shape)\n",
    "        print(batch_labels.shape)\n",
    "        outputs = model(batch_input_ids, batch_attention_mask, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss = total_loss / len(train_df)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e44af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
